@inproceedings { Namkoong2017,
	abstract = {Standard forms of coordinate and stochastic gradient methods do not adapt to structure in data; their good behavior under random sampling is predicated on uniformity in data. When gradients in certain blocks of features (for coordinate descent) or examples (for SGD) are larger than others, there is a natural structure that can be exploited for quicker convergence. Yet adaptive variants often suffer nontrivial computational overhead. We present a framework that discovers and leverages such structural properties at a low computational cost. We employ a bandit optimization procedure that “learns” probabilities for sampling coordinates or examples in (non-smooth) optimization problems, allowing us to guarantee performance close to that of the optimal stationary sampling distribution. When such structures exist, our algorithms achieve tighter convergence guarantees than their non-adaptive counterparts, and we complement our analysis with experiments on several datasets.},
	url = {http://proceedings.mlr.press/v70/namkoong17a.html},
	pdf = {http://proceedings.mlr.press/v70/namkoong17a/namkoong17a.pdf},
	publisher = {PMLR},
	month = {06--11 Aug},
	address = {International Convention Centre, Sydney, Australia},
	series = {Proceedings of Machine Learning Research},
	volume = {70},
	editor = {Doina Precup and Yee Whye Teh},
	year = {2017},
	pages = {2574--2583},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	title = {{Adaptive Sampling Probabilities for Non-Smooth Optimization}},
	author = {Hongseok Namkoong and Aman Sinha and Steve Yadlowsky and John C. Duchi}
}

