@article { Kallus2019a,
	abstract = {Off-policy evaluation (OPE) in reinforcement learning is notoriously
difficult in long- and infinite-horizon settings due to diminishing overlap
between behavior and target policies. In this paper, we study the role of
Markovian, time-invariant, and ergodic structure in efficient OPE. We first
derive the efficiency limits for OPE when one assumes each of these structures.
This precisely characterizes the curse of horizon: in time-variant processes,
OPE is only feasible in the near-on-policy setting, where behavior and target
policies are sufficiently similar. But, in ergodic time-invariant Markov
decision processes, our bounds show that truly-off-policy evaluation is
feasible, even with only just one dependent trajectory, and provide the limits
of how well we could hope to do. We develop a new estimator based on Double
Reinforcement Learning (DRL) that leverages this structure for OPE. Our DRL
estimator simultaneously uses estimated stationary density ratios and
$q$-functions and remains efficient when both are estimated at slow,
nonparametric rates and remains consistent when either is estimated
consistently. We investigate these properties and the performance benefits of
leveraging the problem structure for more efficient OPE.},
	url = {http://arxiv.org/pdf/1909.05850v1},
	eprint = {1909.05850},
	arxivid = {1909.05850},
	archiveprefix = {arXiv},
	month = {Sep},
	year = {2019},
	booktitle = {arXiv},
	title = {{Efficiently Breaking the Curse of Horizon: Double Reinforcement Learning
  in Infinite-Horizon Processes}},
	author = {Nathan Kallus and Masatoshi Uehara}
}

