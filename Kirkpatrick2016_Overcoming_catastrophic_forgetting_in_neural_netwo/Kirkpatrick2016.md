A beautiful set of papers. Main idea is learning a model while not forgetting a previous one. This can be considered in multiple ways: 1) you can try to solve min $L(new) st KL_{oldtask}(old|new)$ small and it actually ends up same as natural gradient descent. So basically while learning one task, you keep track of gradients and computer Fisher information matrix. In the new task, you put a quadratic penalty around this. Another derivation is from Laplace Approximation/Propogation sense. When you solved the old task, you can actually approximate psoterior p(theta|data) as a normal with theta* and the hessian. Practically this becomes a  bayesian way and you forget old data and directly optimize for old posterior + new loss
