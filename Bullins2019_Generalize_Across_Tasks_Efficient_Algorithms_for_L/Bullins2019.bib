@inproceedings { Bullins2019,
	abstract = {We present provable algorithms for learning linear representations which are trained in a supervised
 fashion across a number of tasks. Furthermore, whereas previous methods in the context of multitask learning only allow for generalization within tasks that have already been observed, our
 representations are both efficiently learnable and accompanied by generalization guarantees to
 unseen tasks. Our method relies on a certain convex relaxation of a non-convex problem, making
 it amenable to online learning procedures. We further ensure that a low-rank representation is
 maintained, and we allow for various trade-offs between sample complexity and per-iteration cost,
 depending on the choice of algorithm.},
	url = {http://proceedings.mlr.press/v98/bullins19a.html},
	pdf = {http://proceedings.mlr.press/v98/bullins19a/bullins19a.pdf},
	publisher = {PMLR},
	month = {22--24 Mar},
	address = {Chicago, Illinois},
	series = {Proceedings of Machine Learning Research},
	volume = {98},
	editor = {Garivier, Aur\'elien and Kale, Satyen},
	year = {2019},
	pages = {235--246},
	booktitle = {Proceedings of the 30th International Conference on Algorithmic Learning Theory},
	title = {{Generalize Across Tasks: Efficient Algorithms for Linear
 Representation Learning}},
	author = {Bullins, Brian and Hazan, Elad and Kalai, Adam and Livni, Roi}
}

