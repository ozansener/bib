@article { Bellemare2019,
	abstract = {This paper proposes a new approach to representation learning based on
geometric properties of the space of value functions. We study a two-part
approximation of the value function: a nonlinear map from states to vectors, or
representation, followed by a linear map from vectors to values. Our
formulation considers adapting the representation to minimize the (linear)
approximation of the value function of all stationary policies for a given
environment. We show that this optimization reduces to making accurate
predictions regarding a special class of value functions which we call
adversarial value functions (AVFs). We argue that these AVFs make excellent
auxiliary tasks, and use them to construct a loss which can be efficiently
minimized to find a near-optimal representation for reinforcement learning. We
highlight characteristics of the method in a series of experiments on the
four-room domain.},
	url = {http://arxiv.org/pdf/1901.11530v1},
	eprint = {1901.11530},
	arxivid = {1901.11530},
	archiveprefix = {arXiv},
	month = {Jan},
	year = {2019},
	booktitle = {arXiv},
	title = {{A Geometric Perspective on Optimal Representations for Reinforcement
  Learning}},
	author = {Marc G. Bellemare and Will Dabney and Robert Dadashi and Adrien Ali Taiga and Pablo Samuel Castro and Nicolas Le Roux and Dale Schuurmans and Tor Lattimore and Clare Lyle}
}

