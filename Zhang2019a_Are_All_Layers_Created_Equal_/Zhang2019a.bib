@article { Zhang2019a,
	abstract = {Understanding learning and generalization of deep architectures has been a
major research objective in the recent years with notable theoretical progress.
A main focal point of generalization studies stems from the success of
excessively large networks which defy the classical wisdom of uniform
convergence and learnability. We study empirically the layer-wise functional
structure of over-parameterized deep models. We provide evidence for the
heterogeneous characteristic of layers. To do so, we introduce the notion of
(post training) re-initialization and re-randomization robustness. We show that
layers can be categorized into either "robust" or "critical". In contrast to
critical layers, resetting the robust layers to their initial value has no
negative consequence, and in many cases they barely change throughout training.
Our study provides further evidence that mere parameter counting or norm
accounting is too coarse in studying generalization of deep models.},
	url = {http://arxiv.org/pdf/1902.01996v1},
	eprint = {1902.01996},
	arxivid = {1902.01996},
	archiveprefix = {arXiv},
	month = {Feb},
	year = {2019},
	booktitle = {arXiv},
	title = {{Are All Layers Created Equal?}},
	author = {Chiyuan Zhang and Samy Bengio and Yoram Singer}
}

