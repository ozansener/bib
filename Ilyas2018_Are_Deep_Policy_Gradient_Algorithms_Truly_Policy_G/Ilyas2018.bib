@article { Ilyas2018,
	abstract = {We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. We propose a fine-grained
analysis of state-of-the-art methods based on key aspects of this framework:
gradient estimation, value prediction, optimization landscapes, and trust
region enforcement. We find that from this perspective, the behavior of deep
policy gradient algorithms often deviates from what their motivating framework
would predict. Our analysis suggests first steps towards solidifying the
foundations of these algorithms, and in particular indicates that we may need
to move beyond the current benchmark-centric evaluation methodology.},
	url = {http://arxiv.org/pdf/1811.02553v1},
	eprint = {1811.02553},
	arxivid = {1811.02553},
	archiveprefix = {arXiv},
	month = {Nov},
	year = {2018},
	booktitle = {arXiv},
	title = {{Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?}},
	author = {Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry}
}

