@article { Vezzani2019,
	abstract = {Exploration is an extremely challenging problem in reinforcement learning,
especially in high dimensional state and action spaces and when only sparse
rewards are available. Effective representations can indicate which components
of the state are task relevant and thus reduce the dimensionality of the space
to explore. In this work, we take a representation learning viewpoint on
exploration, utilizing prior experience to learn effective latent
representations, which can subsequently indicate which regions to explore.
Prior experience on separate but related tasks help learn representations of
the state which are effective at predicting instantaneous rewards. These
learned representations can then be used with an entropy-based exploration
method to effectively perform exploration in high dimensional spaces by
effectively lowering the dimensionality of the search space. We show the
benefits of this representation for meta-exploration in a simulated object
pushing environment.},
	url = {http://arxiv.org/pdf/1905.12621v1},
	eprint = {1905.12621},
	arxivid = {1905.12621},
	archiveprefix = {arXiv},
	month = {May},
	year = {2019},
	booktitle = {arXiv},
	title = {{Learning latent state representation for speeding up exploration}},
	author = {Giulia Vezzani and Abhishek Gupta and Lorenzo Natale and Pieter Abbeel}
}

