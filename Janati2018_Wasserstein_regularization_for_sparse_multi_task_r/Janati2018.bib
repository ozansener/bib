@article { Janati2018,
	abstract = {Two important elements have driven recent innovation in the field of
regression: sparsity-inducing regularization, to cope with high-dimensional
problems; multi-task learning through joint parameter estimation, to augment
the number of training samples. Both approaches complement each other in the
sense that a joint estimation results in more samples, which are needed to
estimate sparse models accurately, whereas sparsity promotes models that act on
subsets of related variables. This idea has driven the proposal of block
regularizers such as L1/Lq norms, which however effective, require that active
regressors strictly overlap. In this paper, we propose a more flexible convex
regularizer based on unbalanced optimal transport (OT) theory. That regularizer
promotes parameters that are close, according to the OT geometry, which takes
into account a prior geometric knowledge on the regressor variables. We derive
an efficient algorithm based on a regularized formulation of optimal transport,
which iterates through applications of Sinkhorn's algorithm along with
coordinate descent iterations. The performance of our model is demonstrated on
regular grids and complex triangulated geometries of the cortex with an
application in neuroimaging.},
	url = {http://arxiv.org/pdf/1805.07833v1},
	eprint = {1805.07833},
	arxivid = {1805.07833},
	archiveprefix = {arXiv},
	month = {May},
	year = {2018},
	booktitle = {arXiv},
	title = {{Wasserstein regularization for sparse multi-task regression}},
	author = {Hicham Janati and Marco Cuturi and Alexandre Gramfort}
}

