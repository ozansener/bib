@article { Sajjadi2018,
	abstract = {Recent advances in generative modeling have led to an increased interest in
the study of statistical divergences as means of model comparison. Commonly
used evaluation methods, such as Fr\'echet Inception Distance (FID), correlate
well with the perceived quality of samples and are sensitive to mode dropping.
However, these metrics are unable to distinguish between different failure
cases since they yield one-dimensional scores. We propose a novel definition of
precision and recall for distributions which disentangles the divergence into
two separate dimensions. The proposed notion is intuitive, retains desirable
properties, and naturally leads to an efficient algorithm that can be used to
evaluate generative models. We relate this notion to total variation as well as
to recent evaluation metrics such as Inception Score and FID. To demonstrate
the practical utility of the proposed approach we perform an empirical study on
several variants of Generative Adversarial Networks and the Variational
Autoencoder. In an extensive set of experiments we show that the proposed
metric is able to disentangle the quality of generated samples from the
coverage of the target distribution.},
	url = {http://arxiv.org/pdf/1806.00035v1},
	eprint = {1806.00035},
	arxivid = {1806.00035},
	archiveprefix = {arXiv},
	month = {May},
	year = {2018},
	booktitle = {arXiv},
	title = {{Assessing Generative Models via Precision and Recall}},
	author = {Mehdi S. M. Sajjadi and Olivier Bachem and Mario Lucic and Olivier Bousquet and Sylvain Gelly}
}

