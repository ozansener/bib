@article { Sun2019b,
	abstract = {We introduce a general approach, called test-time training, for improving the
performance of predictive models when test and training data come from
different distributions. Test-time training turns a single unlabeled test
instance into a self-supervised learning problem, on which we update the model
parameters before making a prediction on this instance. We show that this
simple idea leads to surprising improvements on diverse image classification
benchmarks aimed at evaluating robustness to distribution shifts. Theoretical
investigations on a convex model reveal helpful intuitions for when we can
expect our approach to help.},
	url = {http://arxiv.org/pdf/1909.13231v2},
	eprint = {1909.13231},
	arxivid = {1909.13231},
	archiveprefix = {arXiv},
	month = {Sep},
	year = {2019},
	booktitle = {arXiv},
	title = {{Test-Time Training for Out-of-Distribution Generalization}},
	author = {Yu Sun and Xiaolong Wang and Zhuang Liu and John Miller and Alexei A. Efros and Moritz Hardt}
}

