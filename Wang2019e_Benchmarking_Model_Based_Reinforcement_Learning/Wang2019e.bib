@article { Wang2019e,
	abstract = {Model-based reinforcement learning (MBRL) is widely seen as having the
potential to be significantly more sample efficient than model-free RL.
However, research in model-based RL has not been very standardized. It is
fairly common for authors to experiment with self-designed environments, and
there are several separate lines of research, which are sometimes
closed-sourced or not reproducible. Accordingly, it is an open question how
these various existing MBRL algorithms perform relative to each other. To
facilitate research in MBRL, in this paper we gather a wide collection of MBRL
algorithms and propose over 18 benchmarking environments specially designed for
MBRL. We benchmark these algorithms with unified problem settings, including
noisy environments. Beyond cataloguing performance, we explore and unify the
underlying algorithmic differences across MBRL algorithms. We characterize
three key research challenges for future MBRL research: the dynamics
bottleneck, the planning horizon dilemma, and the early-termination dilemma.
Finally, to maximally facilitate future research on MBRL, we open-source our
benchmark in http://www.cs.toronto.edu/~tingwuwang/mbrl.html.},
	url = {http://arxiv.org/pdf/1907.02057v1},
	eprint = {1907.02057},
	arxivid = {1907.02057},
	archiveprefix = {arXiv},
	month = {Jul},
	year = {2019},
	booktitle = {arXiv},
	title = {{Benchmarking Model-Based Reinforcement Learning}},
	author = {Tingwu Wang and Xuchan Bao and Ignasi Clavera and Jerrick Hoang and Yeming Wen and Eric Langlois and Shunshi Zhang and Guodong Zhang and Pieter Abbeel and Jimmy Ba}
}

