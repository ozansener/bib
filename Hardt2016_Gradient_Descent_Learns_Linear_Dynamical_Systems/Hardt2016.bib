@article { Hardt2016,
	abstract = {We prove that stochastic gradient descent efficiently converges to the global
optimizer of the maximum likelihood objective of an unknown linear
time-invariant dynamical system from a sequence of noisy observations generated
by the system. Even though the objective function is non-convex, we provide
polynomial running time and sample complexity bounds under strong but natural
assumptions. Linear systems identification has been studied for many decades,
yet, to the best of our knowledge, these are the first polynomial guarantees
for the problem we consider.},
	url = {http://arxiv.org/pdf/1609.05191v2},
	eprint = {1609.05191},
	arxivid = {1609.05191},
	archiveprefix = {arXiv},
	month = {Sep},
	year = {2016},
	booktitle = {arXiv},
	title = {{Gradient Descent Learns Linear Dynamical Systems}},
	author = {Moritz Hardt and Tengyu Ma and Benjamin Recht}
}

