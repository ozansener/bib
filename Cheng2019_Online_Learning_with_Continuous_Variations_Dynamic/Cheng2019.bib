@article { Cheng2019,
	abstract = {We study the dynamic regret of a new class of online learning problems, in
which the gradient of the loss function changes continuously across rounds with
respect to the learner's decisions. This setup is motivated by the use of
online learning as a tool to analyze the performance of iterative algorithms.
Our goal is to identify interpretable dynamic regret rates that explicitly
consider the loss variations as consequences of the learner's decisions as
opposed to external constraints. We show that achieving sublinear dynamic
regret in general is equivalent to solving certain variational inequalities,
equilibrium problems, and fixed-point problems. Leveraging this identification,
we present necessary and sufficient conditions for the existence of efficient
algorithms that achieve sublinear dynamic regret. Furthermore, we show a
reduction from dynamic regret to both static regret and convergence rate to
equilibriums in the aforementioned problems, which allows us to analyze the
dynamic regret of many existing learning algorithms in few steps.},
	url = {http://arxiv.org/pdf/1902.07286v1},
	eprint = {1902.07286},
	arxivid = {1902.07286},
	archiveprefix = {arXiv},
	month = {Feb},
	year = {2019},
	booktitle = {arXiv},
	title = {{Online Learning with Continuous Variations: Dynamic Regret and
  Reductions}},
	author = {Ching-An Cheng and Jonathan Lee and Ken Goldberg and Byron Boots}
}

