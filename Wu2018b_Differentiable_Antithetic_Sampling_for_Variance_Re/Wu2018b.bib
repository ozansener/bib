@article { Wu2018b,
	abstract = {Stochastic optimization techniques are standard in variational inference
algorithms. These methods estimate gradients by approximating expectations with
independent Monte Carlo samples. In this paper, we explore a technique that
uses correlated, but more representative , samples to reduce estimator
variance. Specifically, we show how to generate antithetic samples that match
sample moments with the true moments of an underlying importance distribution.
Combining a differentiable antithetic sampler with modern stochastic
variational inference, we showcase the effectiveness of this approach for
learning a deep generative model.},
	url = {http://arxiv.org/pdf/1810.02555v1},
	eprint = {1810.02555},
	arxivid = {1810.02555},
	archiveprefix = {arXiv},
	month = {Oct},
	year = {2018},
	booktitle = {arXiv},
	title = {{Differentiable Antithetic Sampling for Variance Reduction in Stochastic
  Variational Inference}},
	author = {Mike Wu and Noah Goodman and Stefano Ermon}
}

