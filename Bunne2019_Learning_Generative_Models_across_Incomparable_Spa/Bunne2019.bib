@article { Bunne2019,
	abstract = {Generative Adversarial Networks have shown remarkable success in learning a
distribution that faithfully recovers a reference distribution in its entirety.
However, in some cases, we may want to only learn some aspects (e.g., cluster
or manifold structure), while modifying others (e.g., style, orientation or
dimension). In this work, we propose an approach to learn generative models
across such incomparable spaces, and demonstrate how to steer the learned
distribution towards target properties. A key component of our model is the
Gromov-Wasserstein distance, a notion of discrepancy that compares
distributions relationally rather than absolutely. While this framework
subsumes current generative models in identically reproducing distributions,
its inherent flexibility allows application to tasks in manifold learning,
relational learning and cross-domain learning.},
	url = {http://arxiv.org/pdf/1905.05461v2},
	eprint = {1905.05461},
	arxivid = {1905.05461},
	archiveprefix = {arXiv},
	month = {May},
	year = {2019},
	booktitle = {arXiv},
	title = {{Learning Generative Models across Incomparable Spaces}},
	author = {Charlotte Bunne and David Alvarez-Melis and Andreas Krause and Stefanie Jegelka}
}

