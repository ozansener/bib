@article { Antoniou2018,
	abstract = {The field of few-shot learning has recently seen substantial advancements.
Most of these advancements came from casting few-shot learning as a
meta-learning problem. Model Agnostic Meta Learning or MAML is currently one of
the best approaches for few-shot learning via meta-learning. MAML is simple,
elegant and very powerful, however, it has a variety of issues, such as being
very sensitive to neural network architectures, often leading to instability
during training, requiring arduous hyperparameter searches to stabilize
training and achieve high generalization and being very computationally
expensive at both training and inference times. In this paper, we propose
various modifications to MAML that not only stabilize the system, but also
substantially improve the generalization performance, convergence speed and
computational overhead of MAML, which we call MAML++.},
	url = {http://arxiv.org/pdf/1810.09502v1},
	eprint = {1810.09502},
	arxivid = {1810.09502},
	archiveprefix = {arXiv},
	month = {Oct},
	year = {2018},
	booktitle = {arXiv},
	title = {{How to train your MAML}},
	author = {Antreas Antoniou and Harrison Edwards and Amos Storkey}
}

