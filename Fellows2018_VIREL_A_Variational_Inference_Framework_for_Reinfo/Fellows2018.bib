@article { Fellows2018,
	abstract = {Applying probabilistic models to reinforcement learning (RL) has become an
exciting direction of research owing to powerful optimisation tools such as
variational inference becoming applicable to RL. However, due to their
formulation, existing inference frameworks and their algorithms pose
significant challenges for learning optimal policies, for example, the absence
of mode capturing behaviour in pseudo-likelihood methods and difficulties in
optimisation of learning objective in maximum entropy RL based approaches. We
propose VIREL, a novel, theoretically grounded probabilistic inference
framework for RL that utilises the action-value function in a parametrised form
to capture future dynamics of the underlying Markov decision process. Owing to
its generality, our framework lends itself to current advances in variational
inference. Applying the variational expectation-maximisation algorithm to our
framework, we show that the actor-critic algorithm can be reduced to
expectation-maximisation. We derive a family of methods from our framework,
including state-of-the-art methods based on soft value functions. We evaluate
two actor-critic algorithms derived from this family, which perform on par with
soft actor critic, demonstrating that our framework offers a promising
perspective on RL as inference.},
	url = {http://arxiv.org/pdf/1811.01132v2},
	eprint = {1811.01132},
	arxivid = {1811.01132},
	archiveprefix = {arXiv},
	month = {Nov},
	year = {2018},
	booktitle = {arXiv},
	title = {{VIREL: A Variational Inference Framework for Reinforcement Learning}},
	author = {Matthew Fellows and Anuj Mahajan and Tim G. J. Rudner and Shimon Whiteson}
}

