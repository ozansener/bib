@article { Wang2016,
	abstract = {This paper presents an actor-critic deep reinforcement learning agent with
experience replay that is stable, sample efficient, and performs remarkably
well on challenging environments, including the discrete 57-game Atari domain
and several continuous control problems. To achieve this, the paper introduces
several innovations, including truncated importance sampling with bias
correction, stochastic dueling network architectures, and a new trust region
policy optimization method.},
	url = {http://arxiv.org/pdf/1611.01224v2},
	eprint = {1611.01224},
	arxivid = {1611.01224},
	archiveprefix = {arXiv},
	month = {Nov},
	year = {2016},
	booktitle = {arXiv},
	title = {{Sample Efficient Actor-Critic with Experience Replay}},
	author = {Ziyu Wang and Victor Bapst and Nicolas Heess and Volodymyr Mnih and Remi Munos and Koray Kavukcuoglu and Nando de Freitas}
}

