@article { Hazan2018,
	abstract = {Suppose an agent is in a (possibly unknown) Markov decision process (MDP) in
the absence of a reward signal, what might we hope that an agent can
efficiently learn to do? One natural, intrinsically defined, objective problem
is for the agent to learn a policy which induces a distribution over state
space that is as uniform as possible, which can be measured in an entropic
sense. Despite the corresponding mathematical program being non-convex, our
main result provides a provably efficient method (both in terms of sample size
and computational complexity) to construct such a maximum-entropy exploratory
policy. Key to our algorithmic methodology is utilizing the conditional
gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an
approximate MDP solver.},
	url = {http://arxiv.org/pdf/1812.02690v1},
	eprint = {1812.02690},
	arxivid = {1812.02690},
	archiveprefix = {arXiv},
	month = {Dec},
	year = {2018},
	booktitle = {arXiv},
	title = {{Provably Efficient Maximum Entropy Exploration}},
	author = {Elad Hazan and Sham M. Kakade and Karan Singh and Abby Van Soest}
}

