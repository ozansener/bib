@article { Bellemare2019b,
	abstract = {We propose a new perspective on representation learning in reinforcement
learning based on geometric properties of the space of value functions. We
leverage this perspective to provide formal evidence regarding the usefulness
of value functions as auxiliary tasks. Our formulation considers adapting the
representation to minimize the (linear) approximation of the value function of
all stationary policies for a given environment. We show that this optimization
reduces to making accurate predictions regarding a special class of value
functions which we call adversarial value functions (AVFs). We demonstrate that
using value functions as auxiliary tasks corresponds to an expected-error
relaxation of our formulation, with AVFs a natural candidate, and identify a
close relationship with proto-value functions (Mahadevan, 2005). We highlight
characteristics of AVFs and their usefulness as auxiliary tasks in a series of
experiments on the four-room domain.},
	url = {http://arxiv.org/pdf/1901.11530v2},
	eprint = {1901.11530},
	arxivid = {1901.11530},
	archiveprefix = {arXiv},
	month = {Jan},
	year = {2019},
	booktitle = {arXiv},
	title = {{A Geometric Perspective on Optimal Representations for Reinforcement
  Learning}},
	author = {Marc G. Bellemare and Will Dabney and Robert Dadashi and Adrien Ali Taiga and Pablo Samuel Castro and Nicolas Le Roux and Dale Schuurmans and Tor Lattimore and Clare Lyle}
}

