@inproceedings { Gupta2017a,
	isbn = {978-3-319-71682-4},
	abstract = {This work considers the problem of learning cooperative policies in complex, partially observable domains without explicit communication. We extend three classes of single-agent deep reinforcement learning algorithms based on policy gradient, temporal-difference error, and actor-critic methods to cooperative multi-agent systems. To effectively scale these algorithms beyond a trivial number of agents, we combine them with a multi-agent variant of curriculum learning. The algorithms are benchmarked on a suite of cooperative control tasks, including tasks with discrete and continuous actions, as well as tasks with dozens of cooperating agents. We report the performance of the algorithms using different neural architectures, training procedures, and reward structures. We show that policy gradient methods tend to outperform both temporal-difference and actor-critic methods and that curriculum learning is vital to scaling reinforcement learning algorithms in complex multi-agent domains.},
	pages = {66--83},
	address = {Cham},
	publisher = {Springer International Publishing},
	year = {2017},
	booktitle = {Autonomous Agents and Multiagent Systems},
	title = {{Cooperative Multi-agent Control Using Deep Reinforcement Learning}},
	editor = {Sukthankar, Gita
and Rodriguez-Aguilar, Juan A.},
	author = {Gupta, Jayesh K.
and Egorov, Maxim
and Kochenderfer, Mykel}
}

