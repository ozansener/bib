@article { Asadulaev2019,
	abstract = {Deep Linear Networks do not have expressive power but they are mathematically
tractable. In our work, we found an architecture in which they are expressive.
This paper presents a Linear Distillation Learning (LDL) a simple remedy to
improve the performance of linear networks through distillation. In deep
learning models, distillation often allows the smaller/shallow network to mimic
the larger models in a much more accurate way, while a network of the same size
trained on the one-hot targets can't achieve comparable results to the
cumbersome model. In our method, we train students to distill teacher
separately for each class in dataset. The most striking result to emerge from
the data is that neural networks without activation functions can achieve high
classification score on a small amount of data on MNIST and Omniglot datasets.
Due to tractability, linear networks can be used to explain some phenomena
observed experimentally in deep non-linear networks. The suggested approach
could become a simple and practical instrument while further studies in the
field of linear networks and distillation are yet to be undertaken.},
	url = {http://arxiv.org/pdf/1906.05431v1},
	eprint = {1906.05431},
	arxivid = {1906.05431},
	archiveprefix = {arXiv},
	month = {Jun},
	year = {2019},
	booktitle = {arXiv},
	title = {{Linear Distillation Learning}},
	author = {Arip Asadulaev and Igor Kuznetsov and Andrey Filchenkov}
}

