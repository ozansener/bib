@article { Hong2018,
	abstract = {Efficient exploration remains a challenging research problem in reinforcement
learning, especially when an environment contains large state spaces, deceptive
local optima, or sparse rewards. To tackle this problem, we present a
diversity-driven approach for exploration, which can be easily combined with
both off- and on-policy reinforcement learning algorithms. We show that by
simply adding a distance measure to the loss function, the proposed methodology
significantly enhances an agent's exploratory behaviors, and thus preventing
the policy from being trapped in local optima. We further propose an adaptive
scaling method for stabilizing the learning process. Our experimental results
in Atari 2600 show that our method outperforms baseline approaches in several
tasks in terms of mean scores and exploration efficiency.},
	url = {http://arxiv.org/pdf/1802.04564v2},
	eprint = {1802.04564},
	arxivid = {1802.04564},
	archiveprefix = {arXiv},
	month = {Feb},
	year = {2018},
	booktitle = {arXiv},
	title = {{Diversity-Driven Exploration Strategy for Deep Reinforcement Learning}},
	author = {Zhang-Wei Hong and Tzu-Yun Shann and Shih-Yang Su and Yi-Hsiang Chang and Chun-Yi Lee}
}

