@article { Hoffman2019,
	abstract = {Design of reliable systems must guarantee stability against input
perturbations. In machine learning, such guarantee entails preventing
overfitting and ensuring robustness of models against corruption of input data.
In order to maximize stability, we analyze and develop a computationally
efficient implementation of Jacobian regularization that increases
classification margins of neural networks. The stabilizing effect of the
Jacobian regularizer leads to significant improvements in robustness, as
measured against both random and adversarial input perturbations, without
severely degrading generalization properties on clean data.},
	url = {http://arxiv.org/pdf/1908.02729v1},
	eprint = {1908.02729},
	arxivid = {1908.02729},
	archiveprefix = {arXiv},
	month = {Aug},
	year = {2019},
	booktitle = {arXiv},
	title = {{Robust Learning with Jacobian Regularization}},
	author = {Judy Hoffman and Daniel A. Roberts and Sho Yaida}
}

