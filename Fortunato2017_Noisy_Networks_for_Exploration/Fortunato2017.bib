@article { Fortunato2017,
	abstract = {We introduce NoisyNet, a deep reinforcement learning agent with parametric
noise added to its weights, and show that the induced stochasticity of the
agent's policy can be used to aid efficient exploration. The parameters of the
noise are learned with gradient descent along with the remaining network
weights. NoisyNet is straightforward to implement and adds little computational
overhead. We find that replacing the conventional exploration heuristics for
A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively)
with NoisyNet yields substantially higher scores for a wide range of Atari
games, in some cases advancing the agent from sub to super-human performance.},
	url = {http://arxiv.org/pdf/1706.10295v2},
	eprint = {1706.10295},
	arxivid = {1706.10295},
	archiveprefix = {arXiv},
	month = {Jun},
	year = {2017},
	booktitle = {arXiv},
	title = {{Noisy Networks for Exploration}},
	author = {Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Ian Osband and Alex Graves and Vlad Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg}
}

