@article { Khodak2019a,
	abstract = {We build a theoretical framework for understanding practical meta-learning
methods that enables the integration of sophisticated formalizations of
task-similarity with the extensive literature on online convex optimization and
sequential prediction algorithms. Our approach enables the task-similarity to
be learned adaptively, provides sharper transfer-risk bounds in the setting of
statistical learning-to-learn, and leads to straightforward derivations of
average-case regret bounds for efficient algorithms in settings where the
task-environment changes dynamically or the tasks share a certain geometric
structure. We use our theory to modify several popular meta-learning algorithms
and improve their training and meta-test-time performance on standard problems
in few-shot and federated deep learning.},
	url = {http://arxiv.org/pdf/1906.02717v2},
	eprint = {1906.02717},
	arxivid = {1906.02717},
	archiveprefix = {arXiv},
	month = {Jun},
	year = {2019},
	booktitle = {arXiv},
	title = {{Adaptive Gradient-Based Meta-Learning Methods}},
	author = {Mikhail Khodak and Maria-Florina Balcan and Ameet Talwalkar}
}

