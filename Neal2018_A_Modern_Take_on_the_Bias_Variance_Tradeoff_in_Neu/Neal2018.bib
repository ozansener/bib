@article { Neal2018,
	abstract = {We revisit the bias-variance tradeoff for neural networks in light of modern
empirical findings. The traditional bias-variance tradeoff in machine learning
suggests that as model complexity grows, variance increases. Classical bounds
in statistical learning theory point to the number of parameters in a model as
a measure of model complexity, which means the tradeoff would indicate that
variance increases with the size of neural networks. However, we empirically
find that variance due to training set sampling is roughly \textit{constant}
(with both width and depth) in practice. Variance caused by the non-convexity
of the loss landscape is different. We find that it decreases with width and
increases with depth, in our setting. We provide theoretical analysis, in a
simplified setting inspired by linear models, that is consistent with our
empirical findings for width. We view bias-variance as a useful lens to study
generalization through and encourage further theoretical explanation from this
perspective.},
	url = {http://arxiv.org/pdf/1810.08591v1},
	eprint = {1810.08591},
	arxivid = {1810.08591},
	archiveprefix = {arXiv},
	month = {Oct},
	year = {2018},
	booktitle = {arXiv},
	title = {{A Modern Take on the Bias-Variance Tradeoff in Neural Networks}},
	author = {Brady Neal and Sarthak Mittal and Aristide Baratin and Vinayak Tantia and Matthew Scicluna and Simon Lacoste-Julien and Ioannis Mitliagkas}
}

