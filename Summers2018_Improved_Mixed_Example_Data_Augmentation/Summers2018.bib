@article { Summers2018,
	abstract = {In order to reduce overfitting, neural networks are typically trained with
data augmentation, the practice of artificially generating additional training
data via label-preserving transformations of existing training examples. Recent
work has demonstrated a surprisingly effective type of non-label-preserving
data augmentation, in which pairs of training examples are averaged together.
In this work, we generalize this "mixed-example data augmentation", which
allows us to find methods that improve upon previous work. This generalization
also reveals that linearity is not necessary as an inductive bias in order for
mixed-example data augmentation to be effective, providing evidence against the
primary theoretical hypothesis from prior work.},
	url = {http://arxiv.org/pdf/1805.11272v2},
	eprint = {1805.11272},
	arxivid = {1805.11272},
	archiveprefix = {arXiv},
	month = {May},
	year = {2018},
	booktitle = {arXiv},
	title = {{Improved Mixed-Example Data Augmentation}},
	author = {Cecilia Summers and Michael J. Dinneen}
}

