@article { Metelli2018,
	abstract = {Policy optimization is an effective reinforcement learning approach to solve
continuous control tasks. Recent achievements have shown that alternating
on-line and off-line optimization is a successful choice for efficient
trajectory reuse. However, deciding when to stop optimizing and collect new
trajectories is non-trivial as it requires to account for the variance of the
objective function estimate. In this paper, we propose a novel model-free
policy search algorithm, POIS, applicable in both control-based and
parameter-based settings. We first derive a high-confidence bound for
importance sampling estimation and then we define a surrogate objective
function which is optimized off-line using a batch of trajectories. Finally,
the algorithm is tested on a selection of continuous control tasks, with both
linear and deep policies, and compared with the state-of-the-art policy
optimization methods.},
	url = {http://arxiv.org/pdf/1809.06098v1},
	eprint = {1809.06098},
	arxivid = {1809.06098},
	archiveprefix = {arXiv},
	month = {Sep},
	year = {2018},
	booktitle = {arXiv},
	title = {{Policy Optimization via Importance Sampling}},
	author = {Alberto Maria Metelli and Matteo Papini and Francesco Faccio and Marcello Restelli}
}

