@article { Pedregosa2016,
	abstract = {Most models in machine learning contain at least one hyperparameter to
control for model complexity. Choosing an appropriate set of hyperparameters is
both crucial in terms of model accuracy and computationally challenging. In
this work we propose an algorithm for the optimization of continuous
hyperparameters using inexact gradient information. An advantage of this method
is that hyperparameters can be updated before model parameters have fully
converged. We also give sufficient conditions for the global convergence of
this method, based on regularity conditions of the involved functions and
summability of errors. Finally, we validate the empirical performance of this
method on the estimation of regularization constants of L2-regularized logistic
regression and kernel Ridge regression. Empirical benchmarks indicate that our
approach is highly competitive with respect to state of the art methods.},
	url = {http://arxiv.org/pdf/1602.02355v5},
	eprint = {1602.02355},
	arxivid = {1602.02355},
	archiveprefix = {arXiv},
	month = {Feb},
	year = {2016},
	booktitle = {arXiv},
	title = {{Hyperparameter optimization with approximate gradient}},
	author = {Fabian Pedregosa}
}

