@article { Majumdar2018,
	abstract = {Our goal is to synthesize controllers for robots that provably generalize
well to novel environments given a dataset of example environments. The key
technical idea behind our approach is to leverage tools from generalization
theory in machine learning by exploiting a precise analogy (which we present in
the form of a reduction) between robustness of controllers to novel
environments and generalization of hypotheses in supervised learning. In
particular, we utilize the Probably Approximately Correct (PAC)-Bayes
framework, which allows us to obtain upper bounds (that hold with high
probability) on the expected cost of (stochastic) controllers across novel
environments. We propose control synthesis algorithms that explicitly seek to
minimize this upper bound. The corresponding optimization problem can be solved
using convex optimization (Relative Entropy Programming in particular) in the
setting where we are optimizing over a finite control policy space. In the more
general setting of continuously parameterized controllers, we minimize this
upper bound using stochastic gradient descent. We present examples of our
approach in the context of obstacle avoidance control with depth measurements.
Our simulated examples demonstrate the potential of our approach to provide
strong generalization guarantees on controllers for robotic systems with
continuous state and action spaces, complicated (e.g., nonlinear) dynamics, and
rich sensory inputs (e.g., depth measurements).},
	url = {http://arxiv.org/pdf/1806.04225v2},
	eprint = {1806.04225},
	arxivid = {1806.04225},
	archiveprefix = {arXiv},
	month = {Jun},
	year = {2018},
	booktitle = {arXiv},
	title = {{PAC-Bayes Control: Synthesizing Controllers that Provably Generalize to
  Novel Environments}},
	author = {Anirudha Majumdar and Maxwell Goldstein}
}

