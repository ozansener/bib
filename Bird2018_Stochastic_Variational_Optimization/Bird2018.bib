@article { Bird2018,
	abstract = {Variational Optimization forms a differentiable upper bound on an objective.
We show that approaches such as Natural Evolution Strategies and Gaussian
Perturbation, are special cases of Variational Optimization in which the
expectations are approximated by Gaussian sampling. These approaches are of
particular interest because they are parallelizable. We calculate the
approximate bias and variance of the corresponding gradient estimators and
demonstrate that using antithetic sampling or a baseline is crucial to mitigate
their problems. We contrast these methods with an alternative parallelizable
method, namely Directional Derivatives. We conclude that, for differentiable
objectives, using Directional Derivatives is preferable to using Variational
Optimization to perform parallel Stochastic Gradient Descent.},
	url = {http://arxiv.org/pdf/1809.04855v1},
	eprint = {1809.04855},
	arxivid = {1809.04855},
	archiveprefix = {arXiv},
	month = {Sep},
	year = {2018},
	booktitle = {arXiv},
	title = {{Stochastic Variational Optimization}},
	author = {Thomas Bird and Julius Kunze and David Barber}
}

