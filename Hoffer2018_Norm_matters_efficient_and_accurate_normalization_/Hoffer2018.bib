@article { Hoffer2018,
	abstract = {Over the past few years batch-normalization has been commonly used in deep
networks, allowing faster training and high performance for a wide variety of
applications. However, the reasons behind its merits remained unanswered, with
several shortcomings that hindered its use for certain tasks. In this work we
present a novel view on the purpose and function of normalization methods and
weight-decay, as tools to decouple weights' norm from the underlying optimized
objective. We also improve the use of weight-normalization and show the
connection between practices such as normalization, weight decay and
learning-rate adjustments. Finally, we suggest several alternatives to the
widely used $L^2$ batch-norm, using normalization in $L^1$ and $L^\infty$
spaces that can substantially improve numerical stability in low-precision
implementations as well as provide computational and memory benefits. We
demonstrate that such methods enable the first batch-norm alternative to work
for half-precision implementations.},
	url = {http://arxiv.org/pdf/1803.01814v2},
	eprint = {1803.01814},
	arxivid = {1803.01814},
	archiveprefix = {arXiv},
	month = {Mar},
	year = {2018},
	booktitle = {arXiv},
	title = {{Norm matters: efficient and accurate normalization schemes in deep
  networks}},
	author = {Elad Hoffer and Ron Banner and Itay Golan and Daniel Soudry}
}

