@article { Liu2018a,
	abstract = {We wish to compute the gradient of an expectation over a finite or countably
infinite sample space having $K \leq \infty$ categories. When $K$ is indeed
infinite, or finite but very large, the relevant summation is intractable.
Accordingly, various stochastic gradient estimators have been proposed. In this
paper, we describe a technique that can be applied to reduce the variance of
any such estimator, without changing its bias---in particular, unbiasedness is
retained. We show that our technique is an instance of Rao-Blackwellization,
and we demonstrate the improvement it yields in empirical studies on both
synthetic and real-world data.},
	url = {http://arxiv.org/pdf/1810.04777v1},
	eprint = {1810.04777},
	arxivid = {1810.04777},
	archiveprefix = {arXiv},
	month = {Oct},
	year = {2018},
	booktitle = {arXiv},
	title = {{Rao-Blackwellized Stochastic Gradients for Discrete Distributions}},
	author = {Runjing Liu and Jeffrey Regier and Nilesh Tripuraneni and Michael I. Jordan and Jon McAuliffe}
}

