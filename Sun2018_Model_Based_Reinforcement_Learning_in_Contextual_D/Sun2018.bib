@article { Sun2018,
	abstract = {We study the sample complexity of model-based reinforcement learning in
general contextual decision processes. We design new algorithms for RL with an
abstract model class and analyze their statistical properties. Our algorithms
have sample complexity governed by a new structural parameter called the
witness rank, which we show to be small in several settings of interest,
including Factored MDPs and reactive POMDPs. We also show that the witness rank
of a problem is never larger than the recently proposed Bellman rank parameter
governing the sample complexity of the model-free algorithm OLIVE (Jiang et
al., 2017), the only other provably sample efficient algorithm at this level of
generality. Focusing on the special case of Factored MDPs, we prove an
exponential lower bound for all model-free approaches, including OLIVE, which
when combined with our algorithmic results demonstrates exponential separation
between model-based and model-free RL in some rich-observation settings.},
	url = {http://arxiv.org/pdf/1811.08540v1},
	eprint = {1811.08540},
	arxivid = {1811.08540},
	archiveprefix = {arXiv},
	month = {Nov},
	year = {2018},
	booktitle = {arXiv},
	title = {{Model-Based Reinforcement Learning in Contextual Decision Processes}},
	author = {Wen Sun and Nan Jiang and Akshay Krishnamurthy and Alekh Agarwal and John Langford}
}

