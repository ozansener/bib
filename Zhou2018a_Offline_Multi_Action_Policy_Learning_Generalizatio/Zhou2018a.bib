@article { Zhou2018a,
	abstract = {In many settings, a decision-maker wishes to learn a rule, or policy, that
maps from observable characteristics of an individual to an action. Examples
include selecting offers, prices, advertisements, or emails to send to
consumers, as well as the problem of determining which medication to prescribe
to a patient. While there is a growing body of literature devoted to this
problem, most existing results are focused on the case where data comes from a
randomized experiment, and further, there are only two possible actions, such
as giving a drug to a patient or not. In this paper, we study the offline
multi-action policy learning problem with observational data and where the
policy may need to respect budget constraints or belong to a restricted policy
class such as decision trees. We build on the theory of efficient
semi-parametric inference in order to propose and implement a policy learning
algorithm that achieves asymptotically minimax-optimal regret. To the best of
our knowledge, this is the first result of this type in the multi-action setup,
and it provides a substantial performance improvement over the existing
learning algorithms. We then consider additional computational challenges that
arise in implementing our method for the case where the policy is restricted to
take the form of a decision tree. We propose two different approaches, one
using a mixed integer program formulation and the other using a tree-search
based algorithm.},
	url = {http://arxiv.org/pdf/1810.04778v1},
	eprint = {1810.04778},
	arxivid = {1810.04778},
	archiveprefix = {arXiv},
	month = {Oct},
	year = {2018},
	booktitle = {arXiv},
	title = {{Offline Multi-Action Policy Learning: Generalization and Optimization}},
	author = {Zhengyuan Zhou and Susan Athey and Stefan Wager}
}

