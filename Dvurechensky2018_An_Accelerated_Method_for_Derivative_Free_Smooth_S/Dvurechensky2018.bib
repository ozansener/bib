@article { Dvurechensky2018,
	abstract = {We consider an unconstrained problem of minimization of a smooth convex
function which is only available through noisy observations of its values, the
noise consisting of two parts. Similar to stochastic optimization problems, the
first part is of a stochastic nature. On the opposite, the second part is an
additive noise of an unknown nature, but bounded in the absolute value. In the
two-point feedback setting, i.e. when pairs of function values are available,
we propose an accelerated derivative-free algorithm together with its
complexity analysis. The complexity bound of our derivative-free algorithm is
only by a factor of $\sqrt{n}$ larger than the bound for accelerated
gradient-based algorithms, where $n$ is the dimension of the decision variable.
We also propose a non-accelerated derivative-free algorithm with a complexity
bound similar to the stochastic-gradient-based algorithm, that is, our bound
does not have any dimension-dependent factor. Interestingly, if the solution of
the problem is sparse, for both our algorithms, we obtain better complexity
bound if the algorithm uses a 1-norm proximal setup, rather than the Euclidean
proximal setup, which is a standard choice for unconstrained problems.},
	url = {http://arxiv.org/pdf/1802.09022v1},
	eprint = {1802.09022},
	arxivid = {1802.09022},
	archiveprefix = {arXiv},
	month = {Feb},
	year = {2018},
	booktitle = {arXiv},
	title = {{An Accelerated Method for Derivative-Free Smooth Stochastic Convex
  Optimization}},
	author = {Pavel Dvurechensky and Alexander Gasnikov and Eduard Gorbunov}
}

