@article { Achille2019,
	abstract = {We introduce an asymmetric distance in the space of learning tasks, and a
framework to compute their complexity. These concepts are foundational to the
practice of transfer learning, ubiquitous in Deep Learning, whereby a
parametric model is pre-trained for a task, and then used for another after
fine-tuning. The framework we develop is intrinsically non-asymptotic,
capturing the finite nature of the training dataset, yet it allows
distinguishing learning from memorization. It encompasses, as special cases,
classical notions from Kolmogorov complexity, Shannon, and Fisher Information.
However, unlike some of those frameworks, it can be applied easily to
large-scale models and real-world datasets. It is the first framework to
explicitly account for the optimization scheme, which plays a crucial role in
Deep Learning, in measuring complexity and information.},
	url = {http://arxiv.org/pdf/1904.03292v1},
	eprint = {1904.03292},
	arxivid = {1904.03292},
	archiveprefix = {arXiv},
	month = {Apr},
	year = {2019},
	booktitle = {arXiv},
	title = {{The Information Complexity of Learning Tasks, their Structure and their
  Distance}},
	author = {Alessandro Achille and Giovanni Paolini and Glen Mbeng and Stefano Soatto}
}

